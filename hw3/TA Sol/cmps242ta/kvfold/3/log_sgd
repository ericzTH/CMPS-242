Stochastic gradient descent training...
The cost of 0th iteration is: 0.693069035149
The norm2 distance of old w and new w is: 0.0113631827149
Stochastic gradient descent training is done!
The best paramters of sgd are:[[  2.03775029e-276   3.59220934e-052   3.91253206e-178 ...,
   -1.18876692e-021  -2.45029341e-021  -4.17879765e-047]]
The convergence time is 1.97612500191seconds.
