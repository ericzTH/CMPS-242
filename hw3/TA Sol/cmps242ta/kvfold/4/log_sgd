Stochastic gradient descent training...
The cost of 0th iteration is: 0.693067651909
The norm2 distance of old w and new w is: 0.0113626457366
Stochastic gradient descent training is done!
The best paramters of sgd are:[[  2.20003397e-276   3.58190351e-052   3.86771579e-178 ...,
   -1.17243172e-021  -2.50913760e-021  -4.13571084e-047]]
The convergence time is 1.93214392662seconds.
