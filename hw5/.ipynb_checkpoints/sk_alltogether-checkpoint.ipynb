{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import codecs,os,re,time,sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "if os.name != 'posix':\n",
    "\tos.chdir(\"M:\\Course stuff\\Fall 17\\CMPS 242\\hw5\")\n",
    "train_data = pd.read_csv(\"train.csv\",header = None)\n",
    "labels_train = pd.read_csv(\"labels_train_tweets.csv\",header = None)\n",
    "labels_train[0] = labels_train[0].map({'HC':[1,0],'DT':[0,1]})\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lexicon.\n",
      "Lexicon has  12881  entries\n",
      "size of TfIdf vocabulary(Number of Unique Words from the data set):  12891 \n",
      "Total number of tweets:  4743 \n",
      "Total tweets in test:  1701\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_steps = 300\n",
    "batch_size = 1\n",
    "\n",
    "text = train_data[0].copy()\n",
    "test_text = test_data['tweet'].copy()\n",
    "\n",
    "def create_token_lexicon(trains):\n",
    "\tprint(\"Creating lexicon.\")\n",
    "\ttknzr = TweetTokenizer()\n",
    "\tlexicon = set()\n",
    "\tfor i in range(len(trains)):\n",
    "\t\tlexicon.update(tknzr.tokenize(trains[i]))\n",
    "\tw_counts = Counter(lexicon)\n",
    "\tprint(\"Lexicon has \", len(lexicon), \" entries\")\n",
    "\t#print(w_counts)\n",
    "\treturn list(lexicon)\n",
    "token_list = create_token_lexicon(text)\n",
    "vocab_dict = {}\n",
    "for i in range(len(token_list)):\n",
    "\tif token_list[i] not in vocab_dict:\n",
    "\t\tvocab_dict[token_list[i]] = i\n",
    "vocab_dict['today.'] = vocab_dict['today']\n",
    "vocab_dict['Increíble.\\nhttps://t.co/PmerodqGzQ'] = vocab_dict['Increíble']\n",
    "vocab_dict['Idaho:'] = vocab_dict['Idaho']\n",
    "vocab_dict['Hawaii:\\nhttps://t.co/MnIlk2l9hP\\nIdaho:\\nhttps://t.co/7y5RxLpZRQ\\nMississippi:\\nhttps://t.co/n43cPeJIqa\\nMichigan:\\nhttps://t.co/GL5JiZbqIc'] = vocab_dict['Hawaii']\n",
    "vocab_dict['Mississippi:'] = vocab_dict['Mississippi']\n",
    "vocab_dict['Michigan'] = vocab_dict['Michigan']\n",
    "vocab_dict['#VoteTrumpMS!'] = vocab_dict['Trump']\n",
    "vocab_dict['#Trump2016'] = vocab_dict['Trump']\n",
    "vocab_dict['#NeverForget\\nhttps://t.co/G5TMAUzy0z'] = vocab_dict['Trump']\n",
    "vocab_dict['#VoteTrumpID!'] = vocab_dict['Trump']\n",
    "vocab_dict['#VoteTrumpHI!'] = vocab_dict['Trump']\n",
    "vocab_dict['#VoteTrumpMI!'] = vocab_dict['Trump']\n",
    "vocab_dict['#VoteTrumpMS!'] = vocab_dict['Trump']\n",
    "vocab_dict['Trump!'] = vocab_dict['Trump']\n",
    "vocab_dict['trump'] = vocab_dict['Trump']\n",
    "vocab_dict['Presidential.'] = vocab_dict['Presidential']\n",
    "vocab_dict['#WheresHillary?'] = vocab_dict['Trump']\n",
    "vocab_dict['#MakeAmericaGreatAgain\\n#Trump2016\\xa0https://t.co/awow5pyn7n'] = vocab_dict['Trump']\n",
    "\n",
    "h_iden = np.identity(len(vocab_dict))\n",
    "print(\"size of TfIdf vocabulary(Number of Unique Words from the data set): \",len(vocab_dict),\n",
    "\t\"\\nTotal number of tweets: \",len(text),\"\\nTotal tweets in test: \",len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LSTM MODEL\n",
    "\n",
    "# creating the model\n",
    "tf.reset_default_graph()\n",
    "# Network Parameters\n",
    "num_input = len(vocab_dict) # number of unique words\n",
    "#timesteps = 32 # timesteps\n",
    "num_hidden = 32 # LSTM Hidden Layer size\n",
    "hidden_unit_size = 16 # Feed Forward NN Hidden Layer size\n",
    "num_classes = 2 # neural network output layer\n",
    "\n",
    "# Define weights\n",
    "with tf.name_scope('rnn'):\n",
    "\thidden_layer_1 = tf.Variable(tf.random_normal([num_hidden, hidden_unit_size]))#*0.01/np.sqrt(num_hidden)\n",
    "\t#hidden_layer_1 = tf.get_variable('h_l',[num_hidden, hidden_unit_size],initializer = tf.contrib.layers.xavier_initializer,partition_info = None)\n",
    "\thidden_out = tf.Variable(tf.random_normal([hidden_unit_size, num_classes]))#*0.01/np.sqrt(hidden_unit_size)\n",
    "\tbiases1 = tf.Variable(tf.random_normal([hidden_unit_size]))\n",
    "\tbiases_out = tf.Variable(tf.random_normal([num_classes]))\n",
    "\tweights = {\n",
    "\t\t'h_l': hidden_layer_1,\n",
    "\t\t'out': hidden_out#*0.01/np.sqrt(hidden_unit_size))\n",
    "\t}\n",
    "\t\n",
    "\tbiases = {\n",
    "\t\t'h_l': biases1,\n",
    "\t\t'out': biases_out\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(indices,weights,biases):\n",
    "\t#print(tf.shape(indices))\n",
    "\tembedding_mat = tf.get_variable('embedding_matrix',[len(vocab_dict),embedding_dims],trainable = True,use_resource = True)\n",
    "\tx = tf.nn.embedding_lookup(embedding_mat,indices) #,[1,tf.shape(indices),embedding_dims])\n",
    "\tlstm_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\t#x = tf.transpose(x)\t\n",
    "\t# Get lstm cell output\n",
    "\toutputs, state = tf.nn.dynamic_rnn(lstm_cell,x,dtype=tf.float32)\n",
    "\t# relu activation for ff nn hidden layer\n",
    "\tprint(\"shape of outputs\",outputs.shape)\n",
    "\t#hidden_layer = tf.nn.dropout(tf.nn.elu(tf.matmul(outputs[-1][:],weights['h_l']) + biases['h_l']),keep_prob = 0.5)\n",
    "\thidden_layer = tf.nn.elu(tf.matmul(outputs[-1][:],weights['h_l']) + biases['h_l'])\n",
    "\t# final sigmoidal output (yhat)\n",
    "\tyhat = tf.reshape(tf.reduce_sum(tf.nn.sigmoid(tf.matmul(hidden_layer,weights['out'])+biases['out']),axis = 0),[1,2])\n",
    "\treturn (yhat)\t\n",
    "embedding_dims = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of outputs (?, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "# graph inputs\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "\tindices = tf.placeholder(tf.int32,shape=[None,1])\n",
    "\tY = tf.placeholder(\"float\", [1, num_classes])\n",
    "\t#last_word_in_tweet = tf.Variable(0)\n",
    "with tf.name_scope('xEntropy'):\n",
    "\tlogits = RNN(indices,weights,biases)\n",
    "\tloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels= Y))\n",
    "\tprediction = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\t\t\n",
    "\ttrain_op = optimizer.minimize(loss_op)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "\t#correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "\taccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1 acc: 0.747839\tLoss: 0.31342 0 hrs 0 mins 26.69 secs\n",
      "\n",
      "Optimization Finished!\n",
      "Total time taken = 0 hours, 0 minutes and 26.8644 seconds\n",
      "113 Presidential. https://t.co/tlfpcbVqkT\n",
      "322 #WheresHillary? Sleeping!!!!!\n",
      "329 Priorities. https://t.co/vcpYClUtxw https://t.co/H9ULZ9f9Hg\n",
      "832 43-39 https://t.co/TkQYb6V2do\n",
      "956 Lights, camera, history.\n",
      "https://t.co/C7XFFZtz8Z\n",
      "1114 \"@ladytsbrug: Lovely interview, Mr. Trump! @realDonaldTrump\" Thanks.\n",
      "1301 #MakeAmericaGreatAgain\n",
      "#Trump2016 https://t.co/awow5pyn7n\n",
      "predicting the data finished!\n",
      "\n",
      "time taken = 0 hours, 0 minutes and 3.6822 seconds\n"
     ]
    }
   ],
   "source": [
    "summary_op = tf.summary.merge([tf.summary.scalar(\"Clinton\",prediction[0][0]),tf.summary.scalar(\"Trump\",prediction[0][1])])\n",
    "\n",
    "summary_op2 = tf.summary.merge([tf.summary.scalar(\"loss\",loss_op)])\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "# Start training\n",
    "with sess.as_default():\n",
    "\tstart = time.time()\n",
    "\t# Run the initializer\n",
    "\tsess.run(init) #initializing all the tf variables\n",
    "\twriter = tf.summary.FileWriter('logs', graph = tf.get_default_graph())\n",
    "\t#for step in range(training_steps):\n",
    "\tfor step in range(1):\n",
    "\t\tt = 0\n",
    "\t\tfailed_tweets = 0\n",
    "\t\tstep_start = time.time()\n",
    "\t\t#for tweet in range(10):\n",
    "\t\tfor tweet in range(len(train_data)):\n",
    "\t\t\t#print(tweet)\n",
    "\t\t\tvalid_words_indices = []\n",
    "\t\t\tfor word in range(len(text[tweet].split(' '))):\n",
    "\t\t\t\tif text[tweet].split(' ')[word] in vocab_dict:\n",
    "\t\t\t\t\tvalid_words_indices.append(vocab_dict[text[tweet].split(' ')[word]])\n",
    "\n",
    "\t\t\tif len(valid_words_indices) == 0:\n",
    "\t\t\t\tfailed_tweets += 1                \n",
    "\t\t\t\t#print(tweet,text[tweet])\n",
    "\t\t\tif len(valid_words_indices)!=0:\n",
    "\n",
    "\t\t\t\tvalid_words_indices = np.asarray(valid_words_indices).reshape(len(valid_words_indices),1)\n",
    "\t\t\t\t_,loss, acc, summary_loss = sess.run([train_op,loss_op, accuracy,summary_op2], feed_dict={indices: valid_words_indices,Y: np.asarray(labels_train[0][tweet]).reshape(1,2)})\t\n",
    "\t\t\t#_, pred, loss, acc, summary_pred, summary_loss = sess.run([train_op,prediction,loss_op, accuracy,summary_op,summary_op2], feed_dict={X: matrix1,Y: np.asarray(labels_train[0][tweet]).reshape(1,2)})\t\n",
    "\t\t\tif acc == 1 :\n",
    "\t\t\t\tt+=1\n",
    "\t\twriter.add_summary(summary_loss,tweet)\n",
    "\t\tstep_end = time.time()\n",
    "\t\tstep_ttl = step_end-step_start\n",
    "\t\tstep_hrs = 0\n",
    "\t\tstep_mins = (step_ttl)/60\n",
    "\t\tif step_mins > 60:\n",
    "\t\t\tstep_hrs = mins/60\n",
    "\t\t\tstep_mins %= 60\n",
    "\t\tstep_secs = (step_ttl)%60\n",
    "\t\tprint(\"Training step \"+str(step+1)+\" acc: %f\"%(t/(tweet+1))+\"\\tLoss: \"+str(loss)+\" %i hrs %i mins %.2f secs\"%(step_hrs,step_mins,step_secs)+\"\\n\")\n",
    "\t\t#print(\"\\nFailed number:\\n \",failed_tweets)\n",
    "\t\tif loss <= 0.30 or t/len(train_data) >= 0.95:\n",
    "\t\t\tbreak;\n",
    "\tend = time.time()\n",
    "\tttl = end-start\n",
    "\thrs = 0\n",
    "\tmins = (ttl)/60\n",
    "\tif mins > 60:\n",
    "\t\thrs = mins/60\n",
    "\t\tmins %= 60\n",
    "\tsecs = (ttl)%60\n",
    "\tprint(\"Optimization Finished!\")\n",
    "\tprint(\"Total time taken = %i hours, %i minutes and %.4f seconds\"%(hrs,mins, secs))\n",
    "#### Testing ####\n",
    "\tpred_hc = []\n",
    "\tpred_dt = []\n",
    "\tstart = time.time()\n",
    "\t#for tweet in range(5):\n",
    "\tfor tweet in range(len(test_data)):\n",
    "\t\tvalid_words_indices = []\n",
    "\t\t#print(tweet)\n",
    "\t\tfor word in range(len(test_text[tweet].split(' '))):\t\n",
    "\t\t\t#if test_text[tweet].split(' ')[word] not in vocab_dict:\n",
    "\t\t\t#\tprint(test_text[tweet].split(' ')[word])\n",
    "\t\t\tif test_text[tweet].split(' ')[word] in vocab_dict:\n",
    "\t\t\t\t#print(test_text[tweet+26].split(' ')[word])\n",
    "\t\t\t\tvalid_words_indices.append(vocab_dict[test_text[tweet].split(' ')[word]])\n",
    "\t\tif len(valid_words_indices) == 0:\n",
    "\t\t\tpred_hc.append(0.001)\n",
    "\t\t\tpred_dt.append(0.999)\n",
    "\t\t\t#failed_tweets += 1                \n",
    "\t\t\tprint(tweet,test_text[tweet])\n",
    "\t\tif len(valid_words_indices) != 0:\n",
    "\t\t\tvalid_words_indices = np.asarray(valid_words_indices).reshape(len(valid_words_indices),1)\n",
    "\t\t\ttemp = sess.run(prediction, feed_dict={indices: valid_words_indices})\n",
    "\t\t\tpred_hc.append(temp[0][0])\n",
    "\t\t\tpred_dt.append(temp[0][1])\n",
    "\t\t#logits_test = sess.run([logits], feed_dict={X: matrix_test})\n",
    "\t\t#store_logits.append(logits_test) # yhats for all test tweet\n",
    "\tend = time.time()\n",
    "\tttl = end-start\n",
    "\thrs = 0\n",
    "\tmins = (ttl)/60\n",
    "\tif mins > 60:\n",
    "\t\thrs = mins/60\n",
    "\t\tmins %= 60\n",
    "\tsecs = (ttl)%60\n",
    "\tprint(\"predicting the data finished!\\n\")\n",
    "\tprint(\"time taken = %i hours, %i minutes and %.4f seconds\"%(hrs,mins, secs))\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#MakeAmericaGreatAgain\\n#Trump2016\\xa0https://t.co/awow5pyn7n']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[1301].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'maga'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-6aeb45548f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maga'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'maga'"
     ]
    }
   ],
   "source": [
    "vocab_dict['maga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
